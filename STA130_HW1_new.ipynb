{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba462e50",
   "metadata": {},
   "source": [
    "Link to ChatGPT transcripts: \n",
    "\n",
    "https://chatgpt.com/share/66e38556-81d0-800d-8162-11786b2c3291\n",
    "\n",
    "https://chatgpt.com/share/66e38585-25e0-800d-8932-658312763f41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa029bac",
   "metadata": {},
   "source": [
    "Link to dataset: https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96929eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape\n",
      "(891, 15)\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   survived     891 non-null    int64  \n",
      " 1   pclass       891 non-null    int64  \n",
      " 2   sex          891 non-null    object \n",
      " 3   age          714 non-null    float64\n",
      " 4   sibsp        891 non-null    int64  \n",
      " 5   parch        891 non-null    int64  \n",
      " 6   fare         891 non-null    float64\n",
      " 7   embarked     889 non-null    object \n",
      " 8   class        891 non-null    object \n",
      " 9   who          891 non-null    object \n",
      " 10  adult_male   891 non-null    bool   \n",
      " 11  deck         203 non-null    object \n",
      " 12  embark_town  889 non-null    object \n",
      " 13  alive        891 non-null    object \n",
      " 14  alone        891 non-null    bool   \n",
      "dtypes: bool(2), float64(2), int64(4), object(7)\n",
      "memory usage: 92.4+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "          survived      pclass   sex         age       sibsp       parch  \\\n",
      "count   891.000000  891.000000   891  714.000000  891.000000  891.000000   \n",
      "unique         NaN         NaN     2         NaN         NaN         NaN   \n",
      "top            NaN         NaN  male         NaN         NaN         NaN   \n",
      "freq           NaN         NaN   577         NaN         NaN         NaN   \n",
      "mean      0.383838    2.308642   NaN   29.699118    0.523008    0.381594   \n",
      "std       0.486592    0.836071   NaN   14.526497    1.102743    0.806057   \n",
      "min       0.000000    1.000000   NaN    0.420000    0.000000    0.000000   \n",
      "25%       0.000000    2.000000   NaN   20.125000    0.000000    0.000000   \n",
      "50%       0.000000    3.000000   NaN   28.000000    0.000000    0.000000   \n",
      "75%       1.000000    3.000000   NaN   38.000000    1.000000    0.000000   \n",
      "max       1.000000    3.000000   NaN   80.000000    8.000000    6.000000   \n",
      "\n",
      "              fare embarked  class  who adult_male deck  embark_town alive  \\\n",
      "count   891.000000      889    891  891        891  203          889   891   \n",
      "unique         NaN        3      3    3          2    7            3     2   \n",
      "top            NaN        S  Third  man       True    C  Southampton    no   \n",
      "freq           NaN      644    491  537        537   59          644   549   \n",
      "mean     32.204208      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "std      49.693429      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "min       0.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "25%       7.910400      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "50%      14.454200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "75%      31.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "max     512.329200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
      "\n",
      "       alone  \n",
      "count    891  \n",
      "unique     2  \n",
      "top     True  \n",
      "freq     537  \n",
      "mean     NaN  \n",
      "std      NaN  \n",
      "min      NaN  \n",
      "25%      NaN  \n",
      "50%      NaN  \n",
      "75%      NaN  \n",
      "max      NaN  \n",
      "\n",
      "Missing Values in Each Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "Value Counts for 'sex' column:\n",
      "sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()\n",
    "print(\"shape\")\n",
    "print(df.shape)\n",
    "\n",
    "#Below is copy/pasted from ChatGPT\n",
    "# Basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Missing values summary\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Value counts for categorical columns (example: 'sex')\n",
    "print(\"\\nValue Counts for 'sex' column:\")\n",
    "print(df['sex'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe58d7",
   "metadata": {},
   "source": [
    "To-do 2.2\n",
    "Observation (Row): Each row in the dataset represents information of a single passenger that went on the Titanic, ie. Their survival status, gender, age, etc.\n",
    "Variable (Column): Each column are a set of categorized data of specific characteristics of each passenger. For instance, a column only contains the gender of passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6cebb",
   "metadata": {},
   "source": [
    "To-do 4:\n",
    "\n",
    "df.shape analyzes all columns whilst df.describe, by default, analyzes numerical columns. If there are non-numeric values in the dataset. df.describe() will have a smaller size than df.shape. However, by inputting an include='all', df.describe is still able to analyze all columns\n",
    "\n",
    "df.shape outputs the amount of rows/columns in the dataset regardless of size. df.describe output the count of non-null values for each numeric value. If there are missing values, it always has smaller count than that of df.shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790cedc1",
   "metadata": {},
   "source": [
    "To-do 5:\n",
    "\n",
    "An attribute is a variable that has an assigned value to it, they are often used to store information. We can access an attribute in the code directly. \n",
    "\n",
    "Method is a system of code related with an object and can make changes to or use with attributes. When executing a method, the method can be used to help output a specific value base on its definition in the shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eeaf5d",
   "metadata": {},
   "source": [
    "To-Do 6:\n",
    "\n",
    "count: number of non-blank entries in the column. Count help show how many data points there are for each column.\n",
    "\n",
    "Mean: The average value of the column that illustrates where numbers are densely distributed around. It is given by the sum of values divided by count.\n",
    "\n",
    "std(Standard Deviation): A measure of the spread of values from the mean. It describes the separation of values in a set of data. A high std value suggests that the data is more spread out, with a lower std value being the opposite and values are closer to the mean.\n",
    "\n",
    "min: Smallest value in the column\n",
    "\n",
    "25%: The first quartile, where the lowest 25% of the data lies\n",
    "\n",
    "50%: The median, where 50% of the data lies\n",
    "\n",
    "75%: The third quartile, where 75% of the data lies\n",
    "\n",
    "max: Largest value in the column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46255bf",
   "metadata": {},
   "source": [
    "To-do 7:\n",
    "\n",
    "1.df.dropna() is more preferred than del df.['col'] when there are multiple missing data across multiple columns. For instance, if a dataset records CO2 emissions, there may be multiple missing value across different columns due to uncollected confidential data, or a country is unable to collect relevant information due to lack of technology or a year data is not applicable.\n",
    "\n",
    "2.del df.['col'] is more preferable when values of a specific column needs to be removed. Either because it contains abundant NaN values or the data contained in that column is redundant. For instance, when analyzing performance of a video game character, defense data of a strength-based attacker is irrelevant when we want to analyze the amount of damage it deals.\n",
    "\n",
    "3.Certain situations may require using both del df.['col'] and df.dropna. Usually organizing data in datasets with both data missing across multiple columns and abundant data missing in one column. For instance, if there is both missing data from past years for all countries and missing data from current years in certain countries for an environmental dataset. It is crucial to remove data from years where there is no data for all countries, then remove any country without a data for many years.\n",
    "\n",
    "4.The below missing value indicates there is 177 people's age, 2 people's depart status/depart town, 688 people's deck level is missing. Since column 'deck' has over 50% of data missing, I need to remove that column individually so it does not remove data of 688 passengers if i use df.dropna. Therefore, I decide to remove the age and embarked_town and embarked column first using df.dropna(). When analyzing the survival data based on these three parameters (whether more people of a certain age is able to survive or if their survival data is related to their departure location), it is crucial to filter out people without relevant information.  After removing the age, embarked_town and embarked variable blanks, I will remove the deck variable so I remove all NaN values. Deck variable is not as relevant for data analysis as the missing data indicates that most people do not have a deck number assigned. The analysis of survival data base on 'deck' may also not be applicable to those without a 'deck' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cde75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
      "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
      "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
      "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
      "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
      "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
      "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
      "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
      "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
      "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
      "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
      "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
      "\n",
      "       who  adult_male deck  embark_town alive  alone  \n",
      "0      man        True  NaN  Southampton    no  False  \n",
      "1    woman       False    C    Cherbourg   yes  False  \n",
      "2    woman       False  NaN  Southampton   yes   True  \n",
      "3    woman       False    C  Southampton   yes  False  \n",
      "4      man        True  NaN  Southampton    no   True  \n",
      "..     ...         ...  ...          ...   ...    ...  \n",
      "886    man        True  NaN  Southampton    no   True  \n",
      "887  woman       False    B  Southampton   yes   True  \n",
      "888  woman       False  NaN  Southampton    no  False  \n",
      "889    man        True    C    Cherbourg   yes   True  \n",
      "890    man        True  NaN   Queenstown    no   True  \n",
      "\n",
      "[891 rows x 15 columns]\n",
      "\n",
      "Missing Values in Each Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Before removal:\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Missing values summary\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68b6727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
      "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
      "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
      "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
      "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
      "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
      "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
      "885         0       3  female  39.0      0      5  29.1250        Q   Third   \n",
      "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
      "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
      "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
      "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
      "\n",
      "       who  adult_male  embark_town alive  alone  \n",
      "0      man        True  Southampton    no  False  \n",
      "1    woman       False    Cherbourg   yes  False  \n",
      "2    woman       False  Southampton   yes   True  \n",
      "3    woman       False  Southampton   yes  False  \n",
      "4      man        True  Southampton    no   True  \n",
      "..     ...         ...          ...   ...    ...  \n",
      "885  woman       False   Queenstown    no  False  \n",
      "886    man        True  Southampton    no   True  \n",
      "887  woman       False  Southampton   yes   True  \n",
      "889    man        True    Cherbourg   yes   True  \n",
      "890    man        True   Queenstown    no   True  \n",
      "\n",
      "[712 rows x 14 columns]\n",
      "\n",
      "Missing Values in Each Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#After removal\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "#first attempt with df.dropna('age','embark_town') does not work and I asked GPT\n",
    "df.new = df.dropna(subset=['age','embark_town','embarked'],inplace=True)\n",
    "\n",
    "#Delete deck column with del df ['column']\n",
    "del df['deck']\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Missing values summary\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c891d6e7",
   "metadata": {},
   "source": [
    "To-Do 8:\n",
    "\n",
    "The method df.groupby(\"col1\")[\"col2\"].describe() summarizes statistics for the col2, a set of data that is categorized using values of column 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "737857e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose BloodPressure SkinThickness Insulin   BMI  \\\n",
      "0            6    148.0            72            35    <NA>  33.6   \n",
      "1            1     85.0            66            29    <NA>  26.6   \n",
      "2            8    183.0            64          <NA>    <NA>  23.3   \n",
      "3            1     89.0            66            23      94  28.1   \n",
      "4            0    137.0            40            35     168  43.1   \n",
      "5            5    116.0            74          <NA>    <NA>  25.6   \n",
      "6            3     78.0            50            32      88   NaN   \n",
      "7           10    115.0          <NA>            35    <NA>  35.3   \n",
      "8            2    197.0            70            45     543  30.5   \n",
      "9            8      NaN            82            31    <NA>  24.0   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "5                     0.201   30        0  \n",
      "6                     0.248   26        1  \n",
      "7                     0.134   29        0  \n",
      "8                     0.158   53        1  \n",
      "9                     0.364   32        0  \n",
      "\n",
      "Missing Values in Each Column:\n",
      "Pregnancies                 0\n",
      "Glucose                     1\n",
      "BloodPressure               1\n",
      "SkinThickness               2\n",
      "Insulin                     6\n",
      "BMI                         1\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n",
      "\\ \n",
      "Demonstration of the method\n",
      "             count   mean       std    min    25%    50%    75%    max\n",
      "Pregnancies                                                           \n",
      "0              1.0  137.0       NaN  137.0  137.0  137.0  137.0  137.0\n",
      "1              2.0   87.0  2.828427   85.0   86.0   87.0   88.0   89.0\n",
      "2              1.0  197.0       NaN  197.0  197.0  197.0  197.0  197.0\n",
      "3              1.0   78.0       NaN   78.0   78.0   78.0   78.0   78.0\n",
      "5              1.0  116.0       NaN  116.0  116.0  116.0  116.0  116.0\n",
      "6              1.0  148.0       NaN  148.0  148.0  148.0  148.0  148.0\n",
      "8              1.0  183.0       NaN  183.0  183.0  183.0  183.0  183.0\n",
      "10             1.0  115.0       NaN  115.0  115.0  115.0  115.0  115.0\n"
     ]
    }
   ],
   "source": [
    "#Example for 8.1\n",
    "\n",
    "import pandas as pd\n",
    "file = \"pima_indians_diabetes_subset.csv\"\n",
    "ds = pd.read_csv(file)\n",
    "\n",
    "#Removing 0 values in variables, 0 stand for a blank value in the original csv file\n",
    "ds[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = ds[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, pd.NA)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "# Missing value summary\n",
    "print(\"\\nMissing Values in Each Column:\")\n",
    "print(ds.isnull().sum())\n",
    "\n",
    "print('\\ ')\n",
    "#visually better lol\n",
    "\n",
    "print('Demonstration of the method')\n",
    "print(ds.groupby(\"Pregnancies\")[\"Glucose\"].describe())\n",
    "#To be honest I think this dataset has too few values to deal with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c17185c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count      mean        std       min       25%       50%       75%  \\\n",
      "age                                                                         \n",
      "0.42     1.0    8.5167        NaN    8.5167    8.5167    8.5167    8.5167   \n",
      "0.67     1.0   14.5000        NaN   14.5000   14.5000   14.5000   14.5000   \n",
      "0.75     2.0   19.2583   0.000000   19.2583   19.2583   19.2583   19.2583   \n",
      "0.83     2.0   23.8750   7.247845   18.7500   21.3125   23.8750   26.4375   \n",
      "0.92     1.0  151.5500        NaN  151.5500  151.5500  151.5500  151.5500   \n",
      "...      ...       ...        ...       ...       ...       ...       ...   \n",
      "70.00    2.0   40.7500  42.779960   10.5000   25.6250   40.7500   55.8750   \n",
      "70.50    1.0    7.7500        NaN    7.7500    7.7500    7.7500    7.7500   \n",
      "71.00    2.0   42.0792  10.500536   34.6542   38.3667   42.0792   45.7917   \n",
      "74.00    1.0    7.7750        NaN    7.7750    7.7750    7.7750    7.7750   \n",
      "80.00    1.0   30.0000        NaN   30.0000   30.0000   30.0000   30.0000   \n",
      "\n",
      "            max  \n",
      "age              \n",
      "0.42     8.5167  \n",
      "0.67    14.5000  \n",
      "0.75    19.2583  \n",
      "0.83    29.0000  \n",
      "0.92   151.5500  \n",
      "...         ...  \n",
      "70.00   71.0000  \n",
      "70.50    7.7500  \n",
      "71.00   49.5042  \n",
      "74.00    7.7750  \n",
      "80.00   30.0000  \n",
      "\n",
      "[88 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#To-do 8.2\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.groupby(\"age\")[\"fare\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376519d7",
   "metadata": {},
   "source": [
    "To-do 8.2\n",
    "\n",
    "df. describe() automatically exclude blanks in each column in the dataset. The count in the method represents the number of non-null values of that column. For df.groupby(\"col1\")[\"col2\"].describe() method, the method counts numbers of non-null values in column2 relative to each col1 group. For instance, for the above code, if col1 = age, then df.groupby(\"col1\")[\"col2\"].describe() method counts the number of non-null values in col2 for each age group.\n",
    "\n",
    "Fundamentally df.describe() summarizes columns separately, the count for this method reflects the number of non-null entries in each column without taking in account of a column's connection with one another. While df.describe treats the dataset as a whole, in df.groupby(\"col1\")[\"col2\"].describe() datasets of col2 are treated as groups of data separated by col1. Then df.groupby(\"col1\")[\"col2\"].describe() is summarizing data of col2 relative to col1. It calculates data for col2 after categorized into each group in col1. This allows df.groupby(\"col1\")[\"col2\"].describe() to focus the relationship between 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723c055",
   "metadata": {},
   "source": [
    "To-do 8.3 Error and troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758c8bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#8.3A\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3A import Error\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()\n",
    "\n",
    "#Both google and chatgpt was able to identify the errors, GPT was able to provide a new code that instructs to execute the import cell hence more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2598f9e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#GPT was able to provide me a correct code to the url for me to execute the code. Google instructed me to read the directory of url carefully only. GPT was more efficient\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanics.csv'"
     ]
    }
   ],
   "source": [
    "#8.3B File not found Error\n",
    "import pandas as pd\n",
    "url = \"titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()\n",
    "\n",
    "#GPT was able to provide me a correct code to the url for me to execute the code. Google instructed me to read the directory of url carefully only. GPT was more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0488b5da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3C Name Error\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()\n",
    "print(ds.groupby(\"age\")[\"pclass\"].describe())\n",
    "\n",
    "#Gpt told me to change the 'ds' in line 6 into df so the code is executed correctly. Google only insisted that ds is not defined. GPT was more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc707f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4083873413.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(url\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "#8.3D Syntax Error\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24803336",
   "metadata": {},
   "source": [
    "8.3D: I did not give GPT a copy of my code, but it was able to read that something was missing and provided me a copy of the correct code. Google returned a general answer on fixing my structure. I think in this case both Google and GPT would be efficient. Google is more efficient for shorter codes like what I have above, since the error is easy to read. In much longer codes GPT would be able to read the specific parts for errors and return a set of correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1daf4cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SeriesGroupBy' object has no attribute 'descrbie'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescrbie\u001b[49m())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1312\u001b[0m, in \u001b[0;36mGroupBy.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[attr]\n\u001b[0;32m-> 1312\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1314\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SeriesGroupBy' object has no attribute 'descrbie'"
     ]
    }
   ],
   "source": [
    "#8.3E Attribute Error\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.groupby(\"sex\")[\"age\"].descrbie())\n",
    "\n",
    "#GPT successfully indicated a typo in describe and gave me a code to fix it. Google was able to give relevant search results on attribute error but did not mention about typos. So GPT was more efficient/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8146671",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Sex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sex'"
     ]
    }
   ],
   "source": [
    "#8.3F Key Error\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.groupby(\"Sex\")[\"age\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36106c",
   "metadata": {},
   "source": [
    "Inputting the error in GPT, that chatbot returned that the column 'Sex' does not exist and fixed that it is actually 'sex' and gave me a correct version of the code. Google did also return relevant results about the key erroe saying there are name errors but GPT was more efficient in helping me identify the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05d8c0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msex\u001b[49m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sex' is not defined"
     ]
    }
   ],
   "source": [
    "#8.3G Key Error\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.groupby(sex)['age'].describe())\n",
    "\n",
    "#GPT successfully identified that i did not use quotations around column 'sex'. Google returned that I may have potential spelling mistakes. GPT is more efficient in locating the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
